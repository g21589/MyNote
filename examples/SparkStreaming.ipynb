{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T16:22:23.477853Z",
     "start_time": "2020-02-25T16:22:23.473822Z"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext, StreamingListener\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import Row, StructField, StringType, IntegerType, FloatType, DoubleType, StructType\n",
    "from pyspark.sql.functions import udf, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T16:38:10.578390Z",
     "start_time": "2020-02-25T16:38:10.489355Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-5DPSB4L:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>appName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=appName>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\\\n",
    "            .setAppName('appName')\\\n",
    "            .setMaster('local[*]')\\\n",
    "            .set(\"spark.executor.memory\", \"2g\")\\\n",
    "            .set(\"spark.cores.max\", \"4\")\\\n",
    "            .set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from io import StringIO, BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = pd.DataFrame({\n",
    "    'A': [0.1,0.2,0.3333333333333333333333333333333333333333],\n",
    "    'B': ['A', 'B', 'C']\n",
    "}, columns=['A', 'B']).to_csv(index=False, sep='|', line_terminator='\\t', float_format='%g')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'JobID': ['00000001'], \n",
    "    'JobData': [job_data]\n",
    "}, columns=['JobID', 'JobData'])\n",
    "\n",
    "df.to_csv('./Data/Job_00000001.csv', header=False, index=False, line_terminator='\\n', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_df = pd.read_csv('./Data/Job_00000001.csv', header=None, names=['JobID', 'JobData'], dtype=str)\n",
    "job_data_df = pd.read_csv(StringIO(row_df.loc[0, 'JobData']), index_col=False, sep='|', lineterminator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    0.633333\n",
       "B         ABC\n",
       "dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_data_df.sum()#.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DStreams (Base on RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T11:07:10.689007Z",
     "start_time": "2020-02-22T11:07:10.636173Z"
    }
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(row):\n",
    "    \n",
    "    # De-serialization\n",
    "    row_df = pd.read_csv(StringIO(row), header=None, names=['JobID', 'JobData'], dtype=str)\n",
    "    job_data_df = pd.read_csv(StringIO(row_df.loc[0, 'JobData']), index_col=False, sep='|', lineterminator='\\t')\n",
    "    \n",
    "    # Do something...\n",
    "    result_dict = {\n",
    "        'JobID': row_df.loc[0, 'JobID'],\n",
    "        'JobResult': 'AAA'#job_data_df.to_dict(orient='records')\n",
    "    }\n",
    "    \n",
    "    return result_dict\n",
    "    \n",
    "result_rdd = ssc.textFileStream('./Data').map(func).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('JobID', StringType(), nullable=True),\n",
    "    StructField('JobResult', StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "def save_process(time, rdd):\n",
    "    count = rdd.count()\n",
    "    print(\"[{:s}] {:d}\".format(str(time), count))\n",
    "    if count > 0:\n",
    "        print(rdd.take(5))\n",
    "        result_sdf = rdd.toDF(schema).cache()\n",
    "        result_sdf.show()\n",
    "    \n",
    "result_rdd.foreachRDD(save_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T11:07:26.826446Z",
     "start_time": "2020-02-22T11:07:26.732497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-03-01 21:26:40] 0\n",
      "[2020-03-01 21:26:45] 1\n",
      "[{'JobID': '00000001', 'JobResult': 'AAA'}]\n",
      "+--------+---------+\n",
      "|   JobID|JobResult|\n",
      "+--------+---------+\n",
      "|00000001|      AAA|\n",
      "+--------+---------+\n",
      "\n",
      "[2020-03-01 21:26:50] 0\n",
      "[2020-03-01 21:26:55] 0\n",
      "[2020-03-01 21:27:00] 0\n",
      "[2020-03-01 21:27:05] 0\n",
      "[2020-03-01 21:27:10] 0\n",
      "[2020-03-01 21:27:15] 0\n",
      "[2020-03-01 21:27:20] 0\n",
      "[2020-03-01 21:27:25] 0\n",
      "[2020-03-01 21:27:30] 0\n",
      "[2020-03-01 21:27:35] 0\n",
      "[2020-03-01 21:27:40] 0\n",
      "[2020-03-01 21:27:45] 0\n",
      "[2020-03-01 21:27:50] 4\n",
      "[{'JobID': '00000001', 'JobResult': 'AAA'}, {'JobID': '00000001', 'JobResult': 'AAA'}, {'JobID': '00000001', 'JobResult': 'AAA'}, {'JobID': '00000001', 'JobResult': 'AAA'}]\n",
      "+--------+---------+\n",
      "|   JobID|JobResult|\n",
      "+--------+---------+\n",
      "|00000001|      AAA|\n",
      "|00000001|      AAA|\n",
      "|00000001|      AAA|\n",
      "|00000001|      AAA|\n",
      "+--------+---------+\n",
      "\n",
      "[2020-03-01 21:27:55] 0\n",
      "[2020-03-01 21:28:00] 0\n",
      "[2020-03-01 21:28:05] 0\n",
      "[2020-03-01 21:28:10] 0\n",
      "[2020-03-01 21:28:15] 0\n",
      "[2020-03-01 21:28:20] 0\n",
      "[2020-03-01 21:28:25] 0\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "is_terminated = ssc.awaitTerminationOrTimeout(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T11:12:40.646690Z",
     "start_time": "2020-02-22T11:12:37.203288Z"
    }
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ssc.awaitTerminationOrTimeout(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T16:36:41.247968Z",
     "start_time": "2020-02-25T16:36:41.242982Z"
    }
   },
   "source": [
    "# Structured Streaming (Base on SparkDataFrame, SparkSQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.readStream.text('./Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def to_upper(s):\n",
    "    \n",
    "    import time\n",
    "    import pandas as pd\n",
    "    \n",
    "    print('-----------')\n",
    "    print(s)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    return (s, s.upper())\n",
    "\n",
    "query = sdf.select(to_upper(\"value\")).writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.groupby(\"value\").max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('A', IntegerType(), nullable=True),\n",
    "    StructField('B', StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data = sdf.selectExpr(\"CAST(value as string) as json\")\\\n",
    "    .select(from_json(\"json\", schema).alias(\"data\"))\\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x1d9060a7508>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data.writeStream\\\n",
    "        .option(\"checkpointLocation\", \"./checkpoint\")\\\n",
    "        .format('console')\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = sdf.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('JobID', StringType(), nullable=True),\n",
    "    StructField('JobData', StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "sdf = spark.readStream.csv('./Data', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"JobID string, JobData string\", PandasUDFType.GROUPED_MAP)\n",
    "def func(pdf):\n",
    "    #result_pdf = pdf.assign(JobResult=pdf.JobData).drop(['JobData'], axis=1)\n",
    "    return pdf\n",
    "\n",
    "query = sdf.groupby(\"JobID\").apply(func).writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.rdd.map(lambda x: x).writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.groupBy(\"JobID\").max().writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, nest_asyncio\n",
    "nest_asyncio.apply() # Fix asyncio bug in IPython (RuntimeError: This event loop is already running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waiting for it ...\n",
      "... got it!\n"
     ]
    }
   ],
   "source": [
    "async def waiter(event):\n",
    "    print('waiting for it ...')\n",
    "    await event.wait()\n",
    "    print('... got it!')\n",
    "\n",
    "async def main():\n",
    "    # Create an Event object.\n",
    "    event = asyncio.Event()\n",
    "\n",
    "    # Spawn a Task to wait until 'event' is set.\n",
    "    waiter_task = asyncio.create_task(waiter(event))\n",
    "\n",
    "    # Sleep for 1 second and set the event.\n",
    "    await asyncio.sleep(1)\n",
    "    event.set()\n",
    "\n",
    "    # Wait until the waiter task is finished.\n",
    "    await waiter_task\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
