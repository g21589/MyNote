{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Multi-model with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T09:57:59.140596Z",
     "start_time": "2019-02-17T09:57:59.136499Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:23:12.214508Z",
     "start_time": "2019-02-04T02:23:12.206531Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelper(BaseEstimator):\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=3, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(\"Running GridSearchCV for '%s'.\" % key)\n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X,y)\n",
    "            self.grid_searches[key] = gs    \n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                 'estimator'  : key,\n",
    "                 'min_score'  : min(scores),\n",
    "                 'max_score'  : max(scores),\n",
    "                 'mean_score' : np.mean(scores),\n",
    "                 'std_score'  : np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(r.reshape(len(params),1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params,all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score', 'max_score', 'std_score']\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X_cancer = breast_cancer.data\n",
    "y_cancer = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:25:38.912674Z",
     "start_time": "2019-02-04T02:25:38.909682Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(X_cancer.shape)\n",
    "print(y_cancer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T01:40:15.060798Z",
     "start_time": "2019-02-04T01:40:15.045839Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "models1 = {\n",
    "    'ExtraTreesClassifier'       : ExtraTreesClassifier(),\n",
    "    'RandomForestClassifier'     : RandomForestClassifier(),\n",
    "    'AdaBoostClassifier'         : AdaBoostClassifier(),\n",
    "    'GradientBoostingClassifier' : GradientBoostingClassifier(),\n",
    "    'SVC'                        : SVC()\n",
    "}\n",
    "\n",
    "params1 = {\n",
    "    'ExtraTreesClassifier': { 'n_estimators': [16, 32] },\n",
    "    'RandomForestClassifier': { 'n_estimators': [16, 32] },\n",
    "    'AdaBoostClassifier': { 'n_estimators': [16, 32] },\n",
    "    'GradientBoostingClassifier': { 'n_estimators': [16, 32], 'learning_rate': [0.8, 1.0] },\n",
    "    'SVC': [\n",
    "        {'kernel': ['linear'], 'C': [1, 10]},\n",
    "        {'kernel': ['rbf'], 'C': [1, 10], 'gamma': [0.001, 0.0001]},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:23:23.724619Z",
     "start_time": "2019-02-04T02:23:16.215700Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "helper1.fit(X_cancer, y_cancer, scoring='f1', n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:14:57.447694Z",
     "start_time": "2019-02-04T02:14:57.422779Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "helper1.score_summary(sort_by='max_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:52:50.764886Z",
     "start_time": "2019-02-04T02:52:40.581483Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "helper1 = EstimatorSelectionHelper(models1, params1)\n",
    "\n",
    "flow = Pipeline([\n",
    "    ('pca', pca),\n",
    "    ('selector', helper1),\n",
    "])\n",
    "\n",
    "flow.fit(X_cancer, y_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T02:39:48.956140Z",
     "start_time": "2019-02-04T02:39:48.935180Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "helper1.score_summary(sort_by='max_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipelinehelper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boston house-prices (regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T08:11:11.600136Z",
     "start_time": "2019-02-17T08:11:11.532332Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn import datasets\n",
    "from pipelinehelper import PipelineHelper\n",
    "\n",
    "X, y = datasets.load_boston(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T08:11:14.893558Z",
     "start_time": "2019-02-17T08:11:13.605499Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T13:42:31.254730Z",
     "start_time": "2019-02-16T13:42:29.172109Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', PipelineHelper([\n",
    "        ('std', StandardScaler()),\n",
    "        ('max', MaxAbsScaler()),\n",
    "    ], include_bypass=True)), # this will produce one setting without scaler\n",
    "    ('regressor', PipelineHelper([\n",
    "        ('rf', RandomForestRegressor()),\n",
    "        ('ada', AdaBoostRegressor()),\n",
    "        ('gb', GradientBoostingRegressor()),\n",
    "    ])),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'scaler__selected_model': pipe.named_steps['scaler'].generate({\n",
    "        'std__with_mean': [True, False],\n",
    "        'std__with_std': [True, False],\n",
    "        # no params for 'max' leads to using standard params\n",
    "    }),\n",
    "    'regressor__selected_model': pipe.named_steps['regressor'].generate({\n",
    "\n",
    "        'rf__n_estimators': [10, 20],\n",
    "\n",
    "        'ada__n_estimators': [10, 20],\n",
    "        \n",
    "        'gb__n_estimators': [10, 20],\n",
    "        #'gb__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "        #'gb__max_features': ['auto', 'sqrt', None],\n",
    "\n",
    "    })\n",
    "}\n",
    "grid = GridSearchCV(pipe, params, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1, return_train_score=True)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.decision_function(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T13:42:48.295984Z",
     "start_time": "2019-02-16T13:42:48.176049Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##  Iris (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T15:41:17.217444Z",
     "start_time": "2019-02-14T15:40:19.783311Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from pipelinehelper import PipelineHelper\n",
    "\n",
    "X, y = datasets.load_iris(True)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', PipelineHelper([\n",
    "        ('std', StandardScaler()),\n",
    "        ('max', MaxAbsScaler()),\n",
    "    ], include_bypass=True)), # this will produce one setting without scaler\n",
    "    ('classifier', PipelineHelper([\n",
    "        ('svm', SVC()),\n",
    "        ('rf', RandomForestClassifier()),\n",
    "        ('ada', AdaBoostClassifier()),\n",
    "        ('gb', GradientBoostingClassifier()),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "        \n",
    "        ('nb_pipe', Pipeline([\n",
    "            # Naivie Bayes needs positive numbers\n",
    "            ('scaler', MinMaxScaler()),\n",
    "            ('nb', MultinomialNB())\n",
    "        ])),\n",
    "    ])),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'scaler__selected_model': pipe.named_steps['scaler'].generate({\n",
    "        'std__with_mean': [True, False],\n",
    "        'std__with_std': [True, False],\n",
    "        # no params for 'max' leads to using standard params\n",
    "    }),\n",
    "    'classifier__selected_model': pipe.named_steps['classifier'].generate({\n",
    "\n",
    "        'svm__C': [0.1, 1.0],\n",
    "        'svm__kernel': ['linear', 'rbf'],\n",
    "\n",
    "        'rf__n_estimators': [10, 20, 50, 100, 150],\n",
    "        'rf__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'rf__min_samples_split' : [2, 5, 10],\n",
    "        'rf__min_samples_leaf' : [1, 2, 4],\n",
    "        'rf__bootstrap': [True, False],\n",
    "\n",
    "        'ada__n_estimators': [10, 20, 40, 100],\n",
    "        'ada__algorithm': ['SAMME', 'SAMME.R'],\n",
    "        \n",
    "        'gb__n_estimators': [10, 20, 50, 100],\n",
    "        'gb__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "        'gb__max_features': ['auto', 'sqrt', None],\n",
    "\n",
    "        'knn__n_neighbors': [2, 3, 5, 7, 10],\n",
    "        'knn__leaf_size':[1,2,3,5],\n",
    "        'knn__weights': ['uniform', 'distance'],\n",
    "        'knn__algorithm': ['auto', 'ball_tree','kd_tree','brute'],\n",
    "\n",
    "        'nb_pipe__nb__fit_prior': [True, False],\n",
    "        'nb_pipe__nb__alpha': [0.1, 0.2],\n",
    "    })\n",
    "}\n",
    "grid = GridSearchCV(pipe, params, scoring='accuracy', verbose=2, n_jobs=-1, return_train_score=True)\n",
    "grid.fit(X, y)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.decision_function(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T15:35:39.582537Z",
     "start_time": "2019-02-14T15:35:39.472615Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(grid.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-14T15:32:28.452525Z",
     "start_time": "2019-02-14T15:32:28.447538Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Custom GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T08:10:45.432295Z",
     "start_time": "2019-02-17T08:10:45.276508Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import math, warnings, operator\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from collections import Mapping, namedtuple, defaultdict, Sequence, Iterable\n",
    "from functools import partial, reduce\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "from sklearn.base import BaseEstimator, is_classifier, clone\n",
    "from sklearn.base import MetaEstimatorMixin\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid, ParameterSampler\n",
    "from sklearn.model_selection._split import check_cv\n",
    "from sklearn.model_selection._validation import _fit_and_score, _aggregate_score_dicts, _score\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.externals.joblib import Parallel, delayed, logger, parallel_backend, register_parallel_backend\n",
    "from sklearn.externals import six\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.fixes import sp_version\n",
    "from sklearn.utils.fixes import MaskedArray\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from sklearn.utils.validation import indexable, check_is_fitted, _is_arraylike, _num_samples\n",
    "from sklearn.utils.metaestimators import if_delegate_has_method, _safe_split\n",
    "from sklearn.utils.deprecation import DeprecationDict\n",
    "from sklearn.metrics.scorer import _check_multimetric_scoring, check_scoring\n",
    "from sklearn.metrics.scorer import mean_squared_error, r2_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T08:23:35.511834Z",
     "start_time": "2019-02-17T08:23:35.491860Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def my_fit_and_score(estimator, X, y, scorer, train, test, verbose,\n",
    "                   parameters, fit_params, return_train_score=False,\n",
    "                   return_parameters=False, return_n_test_samples=False,\n",
    "                   return_times=False, return_estimator=False, return_y=False,\n",
    "                   error_score='raise-deprecating'):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : A single callable or dict mapping scorer name to the callable\n",
    "        If it is a single callable, the return value for ``train_scores`` and\n",
    "        ``test_scores`` is a single float.\n",
    "        For a dict, it should be one mapping the scorer name to the scorer\n",
    "        callable object / function.\n",
    "        The callable object / fn should have signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    train : array-like, shape (n_train_samples,)\n",
    "        Indices of training samples.\n",
    "    test : array-like, shape (n_test_samples,)\n",
    "        Indices of test samples.\n",
    "    verbose : integer\n",
    "        The verbosity level.\n",
    "    error_score : 'raise' | 'raise-deprecating' or numeric\n",
    "        Value to assign to the score if an error occurs in estimator fitting.\n",
    "        If set to 'raise', the error is raised.\n",
    "        If set to 'raise-deprecating', a FutureWarning is printed before the\n",
    "        error is raised.\n",
    "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
    "        does not affect the refit step, which will always raise the error.\n",
    "        Default is 'raise-deprecating' but from version 0.22 it will change\n",
    "        to np.nan.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    fit_params : dict or None\n",
    "        Parameters that will be passed to ``estimator.fit``.\n",
    "    return_train_score : boolean, optional, default: False\n",
    "        Compute and return score on training set.\n",
    "    return_parameters : boolean, optional, default: False\n",
    "        Return parameters that has been used for the estimator.\n",
    "    return_n_test_samples : boolean, optional, default: False\n",
    "        Whether to return the ``n_test_samples``\n",
    "    return_times : boolean, optional, default: False\n",
    "        Whether to return the fit/score times.\n",
    "    return_estimator : boolean, optional, default: False\n",
    "        Whether to return the fitted estimator.\n",
    "    Returns\n",
    "    -------\n",
    "    train_scores : dict of scorer name -> float, optional\n",
    "        Score on training set (for all the scorers),\n",
    "        returned only if `return_train_score` is `True`.\n",
    "    test_scores : dict of scorer name -> float, optional\n",
    "        Score on testing set (for all the scorers).\n",
    "    n_test_samples : int\n",
    "        Number of test samples.\n",
    "    fit_time : float\n",
    "        Time spent for fitting in seconds.\n",
    "    score_time : float\n",
    "        Time spent for scoring in seconds.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    estimator : estimator object\n",
    "        The fitted estimator\n",
    "    \"\"\"\n",
    "    if verbose > 1:\n",
    "        if parameters is None:\n",
    "            msg = ''\n",
    "        else:\n",
    "            msg = '%s' % (', '.join('%s=%s' % (k, v)\n",
    "                          for k, v in parameters.items()))\n",
    "        print(\"[CV] %s %s\" % (msg, (64 - len(msg)) * '.'))\n",
    "\n",
    "    # Adjust length of sample weights\n",
    "    fit_params = fit_params if fit_params is not None else {}\n",
    "    fit_params = dict([(k, _index_param_value(X, v, train))\n",
    "                      for k, v in fit_params.items()])\n",
    "\n",
    "    train_scores = {}\n",
    "    if parameters is not None:\n",
    "        estimator.set_params(**parameters)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    X_train, y_train = _safe_split(estimator, X, y, train)\n",
    "    X_test, y_test = _safe_split(estimator, X, y, test, train)\n",
    "\n",
    "    is_multimetric = not callable(scorer)\n",
    "    n_scorers = len(scorer.keys()) if is_multimetric else 1\n",
    "\n",
    "    try:\n",
    "        if y_train is None:\n",
    "            estimator.fit(X_train, **fit_params)\n",
    "        else:\n",
    "            estimator.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Note fit time as time until error\n",
    "        fit_time = time.time() - start_time\n",
    "        score_time = 0.0\n",
    "        if error_score == 'raise':\n",
    "            raise\n",
    "        elif error_score == 'raise-deprecating':\n",
    "            warnings.warn(\"From version 0.22, errors during fit will result \"\n",
    "                          \"in a cross validation score of NaN by default. Use \"\n",
    "                          \"error_score='raise' if you want an exception \"\n",
    "                          \"raised or error_score=np.nan to adopt the \"\n",
    "                          \"behavior from version 0.22.\",\n",
    "                          FutureWarning)\n",
    "            raise\n",
    "        elif isinstance(error_score, numbers.Number):\n",
    "            if is_multimetric:\n",
    "                test_scores = dict(zip(scorer.keys(),\n",
    "                                   [error_score, ] * n_scorers))\n",
    "                if return_train_score:\n",
    "                    train_scores = dict(zip(scorer.keys(),\n",
    "                                        [error_score, ] * n_scorers))\n",
    "            else:\n",
    "                test_scores = error_score\n",
    "                if return_train_score:\n",
    "                    train_scores = error_score\n",
    "            warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
    "                          \" partition for these parameters will be set to %f. \"\n",
    "                          \"Details: \\n%s\" %\n",
    "                          (error_score, format_exception_only(type(e), e)[0]),\n",
    "                          FitFailedWarning)\n",
    "        else:\n",
    "            raise ValueError(\"error_score must be the string 'raise' or a\"\n",
    "                             \" numeric value. (Hint: if using 'raise', please\"\n",
    "                             \" make sure that it has been spelled correctly.)\")\n",
    "\n",
    "    else:\n",
    "        fit_time = time.time() - start_time\n",
    "        # _score will return dict if is_multimetric is True\n",
    "        test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)\n",
    "        score_time = time.time() - start_time - fit_time\n",
    "        if return_train_score:\n",
    "            train_scores = _score(estimator, X_train, y_train, scorer,\n",
    "                                  is_multimetric)\n",
    "        \n",
    "        if return_y:\n",
    "            y_pred = estimator.predict(X_test)\n",
    "        \n",
    "    if verbose > 2:\n",
    "        if is_multimetric:\n",
    "            for scorer_name, score in test_scores.items():\n",
    "                msg += \", %s=%s\" % (scorer_name, score)\n",
    "        else:\n",
    "            msg += \", score=%s\" % test_scores\n",
    "    if verbose > 1:\n",
    "        total_time = score_time + fit_time\n",
    "        end_msg = \"%s, total=%s\" % (msg, logger.short_format_time(total_time))\n",
    "        print(\"[CV] %s %s\" % ((64 - len(end_msg)) * '.', end_msg))\n",
    "\n",
    "    ret = [train_scores, test_scores] if return_train_score else [test_scores]\n",
    "\n",
    "    if return_n_test_samples:\n",
    "        ret.append(_num_samples(X_test))\n",
    "    if return_times:\n",
    "        ret.extend([fit_time, score_time])\n",
    "    if return_parameters:\n",
    "        ret.append(parameters)\n",
    "    if return_estimator:\n",
    "        ret.append(estimator)\n",
    "    if return_y:\n",
    "        ret.append(np.array(y_test))\n",
    "        ret.append(y_pred)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T11:35:22.566754Z",
     "start_time": "2019-02-17T11:35:22.538807Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class UltraGridSearchCV(GridSearchCV):\n",
    "\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid='warn', refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise-deprecating',\n",
    "                 return_train_score=\"warn\"):\n",
    "        super(UltraGridSearchCV, self).__init__(\n",
    "            estimator=estimator, param_grid = param_grid, scoring=scoring, fit_params=fit_params,\n",
    "            n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,\n",
    "            pre_dispatch=pre_dispatch, error_score=error_score,\n",
    "            return_train_score=return_train_score)\n",
    "\n",
    "    def _get_param_iterator(self):\n",
    "        \"\"\"Return ParameterGrid instance for the given param_grid\"\"\"\n",
    "        return ParameterGrid(self.param_grid)\n",
    "    \n",
    "    def fit(self, X, y=None, groups=None, **fit_params):\n",
    "        \"\"\"Run fit with all sets of parameters.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training vector, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples] or [n_samples, n_output], optional\n",
    "            Target relative to X for classification or regression;\n",
    "            None for unsupervised learning.\n",
    "        groups : array-like, with shape (n_samples,), optional\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        **fit_params : dict of string -> object\n",
    "            Parameters passed to the ``fit`` method of the estimator\n",
    "        \"\"\"\n",
    "\n",
    "        if self.fit_params is not None:\n",
    "            warnings.warn('\"fit_params\" as a constructor argument was '\n",
    "                          'deprecated in version 0.19 and will be removed '\n",
    "                          'in version 0.21. Pass fit parameters to the '\n",
    "                          '\"fit\" method instead.', DeprecationWarning)\n",
    "            if fit_params:\n",
    "                warnings.warn('Ignoring fit_params passed as a constructor '\n",
    "                              'argument in favor of keyword arguments to '\n",
    "                              'the \"fit\" method.', RuntimeWarning)\n",
    "            else:\n",
    "                fit_params = self.fit_params\n",
    "        estimator = self.estimator\n",
    "        cv = check_cv(self.cv, y, classifier=is_classifier(estimator))\n",
    "\n",
    "        scorers, self.multimetric_ = _check_multimetric_scoring(\n",
    "            self.estimator, scoring=self.scoring)\n",
    "\n",
    "        if self.multimetric_:\n",
    "            if self.refit is not False and (\n",
    "                    not isinstance(self.refit, six.string_types) or\n",
    "                    # This will work for both dict / list (tuple)\n",
    "                    self.refit not in scorers):\n",
    "                raise ValueError(\"For multi-metric scoring, the parameter \"\n",
    "                                 \"refit must be set to a scorer key \"\n",
    "                                 \"to refit an estimator with the best \"\n",
    "                                 \"parameter setting on the whole data and \"\n",
    "                                 \"make the best_* attributes \"\n",
    "                                 \"available for that metric. If this is not \"\n",
    "                                 \"needed, refit should be set to False \"\n",
    "                                 \"explicitly. %r was passed.\" % self.refit)\n",
    "            else:\n",
    "                refit_metric = self.refit\n",
    "        else:\n",
    "            refit_metric = 'score'\n",
    "\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_splits = cv.get_n_splits(X, y, groups)\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n",
    "                            pre_dispatch=self.pre_dispatch)\n",
    "\n",
    "        fit_and_score_kwargs = dict(scorer=scorers,\n",
    "                                    fit_params=fit_params,\n",
    "                                    return_train_score=self.return_train_score,\n",
    "                                    return_n_test_samples=True,\n",
    "                                    return_times=True,\n",
    "                                    return_parameters=True,\n",
    "                                    return_estimator=True,\n",
    "                                    return_y=True,\n",
    "                                    error_score=self.error_score,\n",
    "                                    verbose=self.verbose)\n",
    "        results_container = [{}]\n",
    "        with parallel:\n",
    "            all_candidate_params = []\n",
    "            all_out = []\n",
    "\n",
    "            def evaluate_candidates(candidate_params):\n",
    "                candidate_params = list(candidate_params)\n",
    "                n_candidates = len(candidate_params)\n",
    "\n",
    "                if self.verbose > 0:\n",
    "                    print(\"Fitting {0} folds for each of {1} candidates,\"\n",
    "                          \" totalling {2} fits\".format(\n",
    "                              n_splits, n_candidates, n_candidates * n_splits))\n",
    "\n",
    "                out = parallel(delayed(my_fit_and_score)(clone(base_estimator),\n",
    "                                                       X, y,\n",
    "                                                       train=train, test=test,\n",
    "                                                       parameters=parameters,\n",
    "                                                       **fit_and_score_kwargs)\n",
    "                               for (parameters, (train, test)), i\n",
    "                               in zip(\n",
    "                                   product(candidate_params, cv.split(X, y, groups)), \n",
    "                                   tqdm(range(n_candidates * n_splits))\n",
    "                               )\n",
    "                )\n",
    "                \n",
    "                all_candidate_params.extend(candidate_params)\n",
    "                all_out.extend(out)\n",
    "\n",
    "                # XXX: When we drop Python 2 support, we can use nonlocal\n",
    "                # instead of results_container\n",
    "                results_container[0] = self._format_results(\n",
    "                    all_candidate_params, scorers, n_splits, all_out)\n",
    "                return results_container[0]\n",
    "\n",
    "            self._run_search(evaluate_candidates)\n",
    "\n",
    "        results = results_container[0]\n",
    "\n",
    "        # For multi-metric evaluation, store the best_index_, best_params_ and\n",
    "        # best_score_ iff refit is one of the scorer names\n",
    "        # In single metric evaluation, refit_metric is \"score\"\n",
    "        if self.refit or not self.multimetric_:\n",
    "            self.best_index_ = results[\"rank_test_%s\" % refit_metric].argmin()\n",
    "            self.best_params_ = results[\"params\"][self.best_index_]\n",
    "            self.best_score_ = results[\"mean_test_%s\" % refit_metric][\n",
    "                self.best_index_]\n",
    "\n",
    "        if self.refit:\n",
    "            self.best_estimator_ = clone(base_estimator).set_params(\n",
    "                **self.best_params_)\n",
    "            refit_start_time = time.time()\n",
    "            if y is not None:\n",
    "                self.best_estimator_.fit(X, y, **fit_params)\n",
    "            else:\n",
    "                self.best_estimator_.fit(X, **fit_params)\n",
    "            refit_end_time = time.time()\n",
    "            self.refit_time_ = refit_end_time - refit_start_time\n",
    "\n",
    "        # Store the only scorer not as a dict for single metric evaluation\n",
    "        self.scorer_ = scorers if self.multimetric_ else scorers['score']\n",
    "\n",
    "        self.cv_results_ = results\n",
    "        self.n_splits_ = n_splits\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _format_results(self, candidate_params, scorers, n_splits, out):\n",
    "        n_candidates = len(candidate_params)\n",
    "        \n",
    "        # if one choose to see train score, \"out\" will contain train score info\n",
    "        if self.return_train_score:\n",
    "            (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n",
    "             score_time, parameters, the_estimator, y_test, y_pred) = zip(*out)\n",
    "        else:\n",
    "            (test_score_dicts, test_sample_counts, fit_time,\n",
    "             score_time, parameters, the_estimator, y_test, y_pred) = zip(*out)\n",
    "        \n",
    "        # test_score_dicts and train_score dicts are lists of dictionaries and\n",
    "        # we make them into dict of lists\n",
    "        test_scores = _aggregate_score_dicts(test_score_dicts)\n",
    "        if self.return_train_score:\n",
    "            train_scores = _aggregate_score_dicts(train_score_dicts)\n",
    "\n",
    "        # TODO: replace by a dict in 0.21\n",
    "        results = (DeprecationDict() if self.return_train_score == 'warn'\n",
    "                   else {})\n",
    "\n",
    "        def _store(key_name, array, weights=None, splits=False, rank=False):\n",
    "            \"\"\"A small helper to store the scores/times to the cv_results_\"\"\"\n",
    "            # When iterated first by splits, then by parameters\n",
    "            # We want `array` to have `n_candidates` rows and `n_splits` cols.\n",
    "            array = np.array(array, dtype=np.float64).reshape(n_candidates,\n",
    "                                                              n_splits)\n",
    "            if splits:\n",
    "                for split_i in range(n_splits):\n",
    "                    # Uses closure to alter the results\n",
    "                    results[\"split%d_%s\"\n",
    "                            % (split_i, key_name)] = array[:, split_i]\n",
    "\n",
    "            array_means = np.average(array, axis=1, weights=weights)\n",
    "            results['mean_%s' % key_name] = array_means\n",
    "            # Weighted std is not directly available in numpy\n",
    "            array_stds = np.sqrt(np.average((array -\n",
    "                                             array_means[:, np.newaxis]) ** 2,\n",
    "                                            axis=1, weights=weights))\n",
    "            results['std_%s' % key_name] = array_stds\n",
    "\n",
    "            if rank:\n",
    "                results[\"rank_%s\" % key_name] = np.asarray(\n",
    "                    rankdata(-array_means, method='min'), dtype=np.int32)\n",
    "\n",
    "        _store('fit_time', fit_time)\n",
    "        _store('score_time', score_time)\n",
    "        \n",
    "        #################################################################\n",
    "        \n",
    "        y_test_array = np.array(y_test)\n",
    "        y_pred_array = np.array(y_pred)\n",
    "        if y_test_array.dtype == np.object:\n",
    "            y_test_array = y_test_array.reshape(n_candidates, n_splits)\n",
    "            y_pred_array = y_pred_array.reshape(n_candidates, n_splits)\n",
    "            for split_i in range(n_splits):\n",
    "                results[\"split%d_%s\" % (split_i, 'y_test')] = y_test_array[:, split_i]\n",
    "                results[\"split%d_%s\" % (split_i, 'y_pred')] = y_pred_array[:, split_i]\n",
    "        else:\n",
    "            y_test_array = y_test_array.reshape(n_candidates, n_splits, -1)\n",
    "            y_pred_array = y_pred_array.reshape(n_candidates, n_splits, -1)\n",
    "            for split_i in range(n_splits):\n",
    "                results[\"split%d_%s\" % (split_i, 'y_test')] = list(y_test_array[:, split_i, :])\n",
    "                results[\"split%d_%s\" % (split_i, 'y_pred')] = list(y_pred_array[:, split_i, :])\n",
    "    \n",
    "        #################################################################\n",
    "    \n",
    "        # Use one MaskedArray and mask all the places where the param is not\n",
    "        # applicable for that candidate. Use defaultdict as each candidate may\n",
    "        # not contain all the params\n",
    "        param_results = defaultdict(partial(MaskedArray,\n",
    "                                            np.empty(n_candidates,),\n",
    "                                            mask=True,\n",
    "                                            dtype=object))\n",
    "        for cand_i, params in enumerate(candidate_params):\n",
    "            for name, value in params.items():\n",
    "                # An all masked empty array gets created for the key\n",
    "                # `\"param_%s\" % name` at the first occurrence of `name`.\n",
    "                # Setting the value at an index also unmasks that index\n",
    "                param_results[\"param_%s\" % name][cand_i] = value\n",
    "\n",
    "        results.update(param_results)\n",
    "        # Store a list of param dicts at the key 'params'\n",
    "        results['params'] = candidate_params\n",
    "\n",
    "        # NOTE test_sample counts (weights) remain the same for all candidates\n",
    "        test_sample_counts = np.array(test_sample_counts[:n_splits], dtype=np.int)\n",
    "        iid = self.iid\n",
    "        if self.iid == 'warn':\n",
    "            warn = False\n",
    "            for scorer_name in scorers.keys():\n",
    "                scores = test_scores[scorer_name].reshape(n_candidates,\n",
    "                                                          n_splits)\n",
    "                means_weighted = np.average(scores, axis=1,\n",
    "                                            weights=test_sample_counts)\n",
    "                means_unweighted = np.average(scores, axis=1)\n",
    "                if not np.allclose(means_weighted, means_unweighted,\n",
    "                                   rtol=1e-4, atol=1e-4):\n",
    "                    warn = True\n",
    "                    break\n",
    "\n",
    "            if warn:\n",
    "                warnings.warn(\"The default of the `iid` parameter will change \"\n",
    "                              \"from True to False in version 0.22 and will be\"\n",
    "                              \" removed in 0.24. This will change numeric\"\n",
    "                              \" results when test-set sizes are unequal.\",\n",
    "                              DeprecationWarning)\n",
    "            iid = True\n",
    "\n",
    "        for scorer_name in scorers.keys():\n",
    "            # Computed the (weighted) mean and std for test scores alone\n",
    "            _store('test_%s' % scorer_name, test_scores[scorer_name],\n",
    "                   splits=True, rank=True,\n",
    "                   weights=test_sample_counts if iid else None)\n",
    "            if self.return_train_score:\n",
    "                prev_keys = set(results.keys())\n",
    "                _store('train_%s' % scorer_name, train_scores[scorer_name],\n",
    "                       splits=True)\n",
    "                if self.return_train_score == 'warn':\n",
    "                    for key in set(results.keys()) - prev_keys:\n",
    "                        message = (\n",
    "                            'You are accessing a training score ({!r}), '\n",
    "                            'which will not be available by default '\n",
    "                            'any more in 0.21. If you need training scores, '\n",
    "                            'please set return_train_score=True').format(key)\n",
    "                        # warn on key access\n",
    "                        results.add_warning(key, message, FutureWarning)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:23:42.645926Z",
     "start_time": "2019-02-17T16:23:42.480161Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, logging\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ultra_ml.parm_search import UltraGridSearchCV\n",
    "from ultra_ml.pipeline import PipelineHelper\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics.scorer import mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:18:58.728175Z",
     "start_time": "2019-02-17T16:18:58.719200Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = datasets.load_boston(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:20:05.631217Z",
     "start_time": "2019-02-17T16:20:05.623266Z"
    }
   },
   "outputs": [],
   "source": [
    "def all_but_first_column(X):\n",
    "    return X[:, 1:]\n",
    "\n",
    "ft = FunctionTransformer(all_but_first_column)\n",
    "X_out = ft.fit_transform(X)\n",
    "print(X.shape, X_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:19:04.274979Z",
     "start_time": "2019-02-17T16:19:04.271017Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "ct = ColumnTransformer([\n",
    "    (\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
    "    (\"norm2\", Normalizer(norm='l2'), slice(3, 5))\n",
    "])\n",
    "#X = np.array([[0.0, 1.0, 2.0, 2.0, 5.0],\n",
    "#              [1.0, 1.0, 0.0, 1.0, -2.0]])\n",
    "#ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:19:06.789870Z",
     "start_time": "2019-02-17T16:19:06.766932Z"
    }
   },
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(\n",
    "    n_splits=3, \n",
    "#    max_train_size=30\n",
    ")\n",
    "tscv\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN:\", train_index, len(train_index))\n",
    "    print(\"TEST:\", test_index, len(test_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:20:24.087494Z",
     "start_time": "2019-02-17T16:20:10.327131Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', PipelineHelper([\n",
    "        ('std', StandardScaler()),\n",
    "        ('max', MaxAbsScaler()),\n",
    "    ], include_bypass=True)), # this will produce one setting without scaler\n",
    "    #('ct', ct),\n",
    "    ('regressor', PipelineHelper([\n",
    "        ('rf', RandomForestRegressor()),\n",
    "        ('xgb', XGBRegressor()),\n",
    "        ('gb', GradientBoostingRegressor()),\n",
    "    ])),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'scaler__selected_model': pipe.named_steps['scaler'].generate({\n",
    "        'std__with_mean': [True, False],\n",
    "        'std__with_std': [True, False],\n",
    "    }),\n",
    "    'regressor__selected_model': pipe.named_steps['regressor'].generate({\n",
    "        'rf__n_estimators': [10, 20, 50, 100],\n",
    "        'xgb__n_estimators': [10, 20, 50, 100],\n",
    "        'gb__n_estimators': [10, 20, 50, 100],\n",
    "        'gb__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "        'gb__max_features': ['auto', 'sqrt', None],\n",
    "    })\n",
    "}\n",
    "\n",
    "scorers = {\n",
    "    'MSE' : make_scorer(mean_squared_error, greater_is_better=False),\n",
    "    'R2'  : make_scorer(r2_score)\n",
    "}\n",
    "\n",
    "grid = UltraGridSearchCV(\n",
    "    estimator  = pipe, \n",
    "    param_grid = params,\n",
    "    cv         = tscv,\n",
    "    scoring    = scorers,\n",
    "    refit      = 'MSE',\n",
    "    verbose    = 20, \n",
    "    n_jobs     = -1, \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "fit_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T10:00:20.156962Z",
     "start_time": "2019-02-17T10:00:20.153997Z"
    }
   },
   "outputs": [],
   "source": [
    "#grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:22:00.715875Z",
     "start_time": "2019-02-17T16:22:00.494674Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(grid.cv_results_).sort_values('mean_test_R2', ascending=False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T11:53:16.030156Z",
     "start_time": "2019-02-17T11:53:16.025196Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df['params'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T11:55:46.158142Z",
     "start_time": "2019-02-17T11:55:46.153182Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:21:11.387569Z",
     "start_time": "2019-02-17T16:21:11.375601Z"
    }
   },
   "outputs": [],
   "source": [
    "for k, v in pipe.named_steps.items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:24:06.565683Z",
     "start_time": "2019-02-17T16:23:50.088049Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_regression_result(result_df, cv):\n",
    "    result_size = len(result_df)\n",
    "    ncols = 4\n",
    "    nrows = int(math.ceil(result_size / float(ncols)))\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12, nrows*1.5), dpi=96)\n",
    "\n",
    "    fig.subplots_adjust(left=0.0, right=1.0, hspace=0.6, wspace=0.1)\n",
    "\n",
    "    for ax, (idx, row) in zip(axs.flat[:result_size], result_df.iterrows()):\n",
    "        \n",
    "        y1 = np.concatenate([row[\"split{}_{}\".format(i, 'y_test')] for i in range(cv.n_splits)])\n",
    "        y2 = np.concatenate([row[\"split{}_{}\".format(i, 'y_pred')] for i in range(cv.n_splits)])\n",
    "        x = np.array(range(len(y2)))\n",
    "        \n",
    "        y2_df = pd.DataFrame({'y2': y2})\n",
    "        move_window = y2_df['y2'].rolling(window=30)\n",
    "        y2_df['UB'] = move_window.mean() + 2 * move_window.std()\n",
    "        y2_df['LB'] = move_window.mean() - 2 * move_window.std()\n",
    "        \n",
    "        ax.plot(x, y1, linewidth=1, label='Actual')\n",
    "        ax.plot(x, y2, linewidth=1, label='Pred')\n",
    "        ax.fill_between(x, y2_df['LB'], y2_df['UB'], alpha=0.33, edgecolor='#CC4F1B', facecolor='#FF9848',\n",
    "                            linewidth=0.5, antialiased=True)\n",
    "\n",
    "        ax.grid()\n",
    "        ax.legend(markerscale=0.5, fontsize=5, labelspacing=0.1, borderpad=0.25)\n",
    "        ax.tick_params(direction='in', labelsize=8, length=2, width=1, pad=3, grid_alpha=0.5)\n",
    "\n",
    "        title_text = '\\n'.join([\n",
    "            str(row['param_regressor__selected_model'])[0:50],\n",
    "            str(row['param_scaler__selected_model'])[0:50]\n",
    "        ])\n",
    "        ax.set_title(title_text, fontsize=7)\n",
    "\n",
    "        ax.text(10, 60, \"R2: %.4f\" % (row['mean_test_R2']), \n",
    "                size=8, rotation=15., ha=\"center\", va=\"center\",\n",
    "                bbox=dict(boxstyle=\"round\", ec=(1., 0.5, 0.5), fc=(1., 0.8, 0.8), alpha=0.8))\n",
    "\n",
    "    # For rest ax\n",
    "    for ax in axs.flat[result_size:]:\n",
    "        ax.grid()\n",
    "        ax.tick_params(direction='in', labelsize=6, length=2, width=1, pad=3, grid_alpha=0.5)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_regression_result(result_df, tscv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-16T16:36:10.484423Z",
     "start_time": "2019-02-16T16:36:08.470348Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "result = Parallel(n_jobs=2)(delayed(sqrt)(i ** 2) for i in tqdm(range(100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T08:14:43.090385Z",
     "start_time": "2019-02-17T08:14:43.084427Z"
    }
   },
   "outputs": [],
   "source": [
    "for a in (np.array([1,2]),np.array([3,5,6])):\n",
    "    print(a.shape)\n",
    "    \n",
    "#np.vstack( (np.array([1,2,5]),np.array([3,5,6])) )\n",
    "b = np.array( (np.array([1,2]), np.array([3,5,6])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-17T16:15:22.122551Z",
     "start_time": "2019-02-17T16:15:22.118559Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "74px",
    "width": "166px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
